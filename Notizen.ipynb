{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNf96qwsGFMGQc9cgrFi7x0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emgrimm42/NLP_Modules/blob/main/Notizen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 1 - 7.11\n",
        "---\n",
        "Thema des Seminars: Fake News in Zeitungen, insbesondere im Bezug auf Naturkatasthrophen  \n",
        "Arbeit mit dem deutschen Zeitungsarchiv als Primärquelle\n",
        "\n",
        "### Learning objectives\n",
        "- Python\n",
        "  - Packages anwenden\n",
        "  - Daten analysieren\n",
        "- Große Datenpackete für Analyse vorbereiten\n",
        "- Transformermodelle und LargeLanguageModels für NLP Aufgaben mit großen Quellenmengen\n",
        "- Kritische reflektion verschiedener Methoden\n",
        "- Schreiben einer HA, 8 Seiten zu einem eigenen Forschungsthema\n",
        "\n",
        "\n",
        "### Types of NLP tasks\n",
        "- Analyse: verstehen und extrahieren von information aus einem Text\n",
        "  - Token and sentence splitting (Wo endet ein Wort/Satz?)\n",
        "  - Stemming and lemmatization (Wortstamm ermittlung und Wörter in ihren Wortstamm umwandeln)\n",
        "  - Part-of-speech tagging (Nomen, Verb, etc. erkennung)\n",
        "  - Named/Numeric entity recognition (z.B. Personenerkennung)\n",
        "  - Topic/Sentiment/Spam classification (welche Gefühle/Intentionen stehen hinter einem Text)  \n",
        "-> Alle Nötig damit der Computer Sprache 'erkennen' und bearbeiten kann\n",
        "- Synthese: Information nehmen und in bedeutungsvollen Text verwandeln\n",
        "\n",
        "Um Notebooks zu speichern eine Kopie machen und in google drive speichern oder auf Gitlab hochladen\n",
        "\n",
        "Pandas ein wichtiges Python pachage um z.B. Daten darzustellen, das excel von Python\n",
        "\n",
        "In HAs nicht das funktioniert nicht, sondern wir haben hier Herausforderungen, die noch gelöst werden müssen"
      ],
      "metadata": {
        "id": "E9Yypb1lnoU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 3 - 14.11.2025\n",
        "---\n",
        "Deutsches Zeitungsarchiv\n",
        "- größtes (erstes) gesamtdeutsches Zeitungsportal\n",
        "- große Regionale Unterschiede in der Abdeckung\n",
        "- Rückgriff auf andere Zeitungsportale um Regionale Lücken abzudecken\n",
        "\n",
        "API - Schnittstellen um Datenzugriff zu ermöglichen, ermöglicht das Runterladen von Daten\n",
        "\n",
        "Distance Suche:\n",
        "zwei Wörter inerhalb einer bestimmten Distanz, z.B.\n",
        "\"China ~ Erdbeben ~\"~60“ - China und Erdbeben müssen innerhalb von 60 Worten voneinander auftauchen\n",
        "\n",
        "Distant reading/scalable reading\n",
        "- immer zurück zu den orginaldaten\n",
        "- Quellenkritik\n",
        "- keine vorzeitigen Schlüsse ziehen"
      ],
      "metadata": {
        "id": "Ad2JJAuGqZpi"
      }
    }
  ]
}