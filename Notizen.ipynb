{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMPGxXJGaTbaomWNLzy1rsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emgrimm42/NLP_Modules/blob/main/Notizen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 1 - 7.11\n",
        "---\n",
        "Thema des Seminars: Fake News in Zeitungen, insbesondere im Bezug auf Naturkatasthrophen  \n",
        "Arbeit mit dem deutschen Zeitungsarchiv als Primärquelle\n",
        "\n",
        "### Learning objectives\n",
        "- Python\n",
        "  - Packages anwenden\n",
        "  - Daten analysieren\n",
        "- Große Datenpackete für Analyse vorbereiten\n",
        "- Transformermodelle und LargeLanguageModels für NLP Aufgaben mit großen Quellenmengen\n",
        "- Kritische reflektion verschiedener Methoden\n",
        "- Schreiben einer HA, 8 Seiten zu einem eigenen Forschungsthema\n",
        "\n",
        "\n",
        "### Types of NLP tasks\n",
        "- Analyse: verstehen und extrahieren von information aus einem Text\n",
        "  - Token and sentence splitting (Wo endet ein Wort/Satz?)\n",
        "  - Stemming and lemmatization (Wortstamm ermittlung und Wörter in ihren Wortstamm umwandeln)\n",
        "  - Part-of-speech tagging (Nomen, Verb, etc. erkennung)\n",
        "  - Named/Numeric entity recognition (z.B. Personenerkennung)\n",
        "  - Topic/Sentiment/Spam classification (welche Gefühle/Intentionen stehen hinter einem Text)  \n",
        "-> Alle Nötig damit der Computer Sprache 'erkennen' und bearbeiten kann\n",
        "- Synthese: Information nehmen und in bedeutungsvollen Text verwandeln\n",
        "\n",
        "Um Notebooks zu speichern eine Kopie machen und in google drive speichern oder auf Gitlab hochladen\n",
        "\n",
        "Pandas ein wichtiges Python pachage um z.B. Daten darzustellen, das excel von Python\n",
        "\n",
        "In HAs nicht das funktioniert nicht, sondern wir haben hier Herausforderungen, die noch gelöst werden müssen"
      ],
      "metadata": {
        "id": "E9Yypb1lnoU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 3 - 14.11.2025\n",
        "---\n",
        "Deutsches Zeitungsarchiv\n",
        "- größtes (erstes) gesamtdeutsches Zeitungsportal\n",
        "- große Regionale Unterschiede in der Abdeckung\n",
        "- Rückgriff auf andere Zeitungsportale um Regionale Lücken abzudecken\n",
        "\n",
        "API - Schnittstellen um Datenzugriff zu ermöglichen, ermöglicht das Runterladen von Daten\n",
        "\n",
        "Distance Suche:\n",
        "zwei Wörter inerhalb einer bestimmten Distanz, z.B.\n",
        "\"China ~ Erdbeben ~\"~60“ - China und Erdbeben müssen innerhalb von 60 Worten voneinander auftauchen\n",
        "\n",
        "Distant reading/scalable reading\n",
        "- immer zurück zu den orginaldaten\n",
        "- Quellenkritik\n",
        "- keine vorzeitigen Schlüsse ziehen"
      ],
      "metadata": {
        "id": "Ad2JJAuGqZpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 4 - 21.11.2025\n",
        "---\n",
        "### Student Paper\n",
        "Themen:\n",
        "- \"Lügenpresse\", \"Zeitungsente\", \"Narchichten Ente\", \"Falschmeldung, Falschnachrichten\"\n",
        "Eine Art anzuschauen wie historisch mit fake news umgegenagen wurde ist solche Begriffe anzuschauen\n",
        "- Oder von einem konkreten historischen Ereignis ausgehend, sich ein Event anschauen, schauen wie kann man hier erkennen/erforschen wie fake news berichtet wurden\n",
        "- paper reflektiert was gemacht wurde, was ich gefunden habe\n",
        "- Kritische Reflexion\n",
        "- Notebooks in Github account\n",
        "\n",
        "#### Paper Aufbau:\n",
        "1. Einleitung\n",
        "   - Thema/Kontext/Forschungsliteratur\n",
        "   - Forschungsfrage\n",
        "   - Forschungsfrage\n",
        "   - Hypothese\n",
        "2. Methode\n",
        "   - Methode zur beantwortung der Forschungsfrage\n",
        "   - Methodisches Vorgehen\n",
        "3. Analyse\n",
        "4. Reflektion der Analyseergebnissse\n",
        "5. Schluss\n",
        "\n",
        "Mögliches Ergebnis: mit den Methoden die ich hatte bin ich gar nicht zu so vielen aussagekräftigen Ergebnissen gekommen\n",
        "\n",
        "Fake News: Propaganda, unbewusst falsche Berichtserstattung, was ist die Intension?\n",
        "\n",
        "## Transformer Modelle\n",
        "Semantic - die Bedeutung von Wörtern im Kontext\n",
        "Transformer Modelle sind kleine Sprachmodelle, lernen Sprache im Kontext\n",
        "encoder - können Sprache verstehen aber nicht produzieren\n",
        "decoder - können Sprache verstehen und produzieren\n",
        "\n",
        "Vorhersehen welches Wort warscheinlich als nächstes kommt\n",
        "\n",
        "Hugging face ist ganz eng mit hugging face verbunden\n",
        "Modelle für Sentiment analyse via google auf hugging face suchen\n",
        "dann imprtieren, auf qualität des Modells achten"
      ],
      "metadata": {
        "id": "dwACYE_vURJ_"
      }
    }
  ]
}