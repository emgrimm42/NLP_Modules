{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYXUjOtMmuibNJWC3WRnmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emgrimm42/NLP_Modules/blob/main/Notizen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 1 - 7.11\n",
        "---\n",
        "Thema des Seminars: Fake News in Zeitungen, insbesondere im Bezug auf Naturkatasthrophen  \n",
        "Arbeit mit dem deutschen Zeitungsarchiv als Primärquelle\n",
        "\n",
        "### Learning objectives\n",
        "- Python\n",
        "  - Packages anwenden\n",
        "  - Daten analysieren\n",
        "- Große Datenpackete für Analyse vorbereiten\n",
        "- Transformermodelle und LargeLanguageModels für NLP Aufgaben mit großen Quellenmengen\n",
        "- Kritische reflektion verschiedener Methoden\n",
        "- Schreiben einer HA, 8 Seiten zu einem eigenen Forschungsthema\n",
        "\n",
        "\n",
        "### Types of NLP tasks\n",
        "- Analyse: verstehen und extrahieren von information aus einem Text\n",
        "  - Token and sentence splitting (Wo endet ein Wort/Satz?)\n",
        "  - Stemming and lemmatization (Wortstamm ermittlung und Wörter in ihren Wortstamm umwandeln)\n",
        "  - Part-of-speech tagging (Nomen, Verb, etc. erkennung)\n",
        "  - Named/Numeric entity recognition (z.B. Personenerkennung)\n",
        "  - Topic/Sentiment/Spam classification (welche Gefühle/Intentionen stehen hinter einem Text)  \n",
        "-> Alle Nötig damit der Computer Sprache 'erkennen' und bearbeiten kann\n",
        "- Synthese: Information nehmen und in bedeutungsvollen Text verwandeln\n",
        "\n",
        "Um Notebooks zu speichern eine Kopie machen und in google drive speichern oder auf Gitlab hochladen\n",
        "\n",
        "Pandas ein wichtiges Python pachage um z.B. Daten darzustellen, das excel von Python\n",
        "\n",
        "In HAs nicht das funktioniert nicht, sondern wir haben hier Herausforderungen, die noch gelöst werden müssen"
      ],
      "metadata": {
        "id": "E9Yypb1lnoU7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 3 - 14.11.2025\n",
        "---\n",
        "Deutsches Zeitungsarchiv\n",
        "- größtes (erstes) gesamtdeutsches Zeitungsportal\n",
        "- große Regionale Unterschiede in der Abdeckung\n",
        "- Rückgriff auf andere Zeitungsportale um Regionale Lücken abzudecken\n",
        "\n",
        "API - Schnittstellen um Datenzugriff zu ermöglichen, ermöglicht das Runterladen von Daten\n",
        "\n",
        "Distance Suche:\n",
        "zwei Wörter inerhalb einer bestimmten Distanz, z.B.\n",
        "\"China ~ Erdbeben ~\"~60“ - China und Erdbeben müssen innerhalb von 60 Worten voneinander auftauchen\n",
        "\n",
        "Distant reading/scalable reading\n",
        "- immer zurück zu den orginaldaten\n",
        "- Quellenkritik\n",
        "- keine vorzeitigen Schlüsse ziehen"
      ],
      "metadata": {
        "id": "Ad2JJAuGqZpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 4 - 21.11.2025\n",
        "---\n",
        "### Student Paper\n",
        "Themen:\n",
        "- \"Lügenpresse\", \"Zeitungsente\", \"Narchichten Ente\", \"Falschmeldung, Falschnachrichten\"\n",
        "Eine Art anzuschauen wie historisch mit fake news umgegenagen wurde ist solche Begriffe anzuschauen\n",
        "- Oder von einem konkreten historischen Ereignis ausgehend, sich ein Event anschauen, schauen wie kann man hier erkennen/erforschen wie fake news berichtet wurden\n",
        "- paper reflektiert was gemacht wurde, was ich gefunden habe\n",
        "- Kritische Reflexion\n",
        "- Notebooks in Github account\n",
        "\n",
        "#### Paper Aufbau:\n",
        "1. Einleitung\n",
        "   - Thema/Kontext/Forschungsliteratur\n",
        "   - Forschungsfrage\n",
        "   - Forschungsfrage\n",
        "   - Hypothese\n",
        "2. Methode\n",
        "   - Methode zur beantwortung der Forschungsfrage\n",
        "   - Methodisches Vorgehen\n",
        "3. Analyse\n",
        "4. Reflektion der Analyseergebnissse\n",
        "5. Schluss\n",
        "\n",
        "Mögliches Ergebnis: mit den Methoden die ich hatte bin ich gar nicht zu so vielen aussagekräftigen Ergebnissen gekommen\n",
        "\n",
        "Fake News: Propaganda, unbewusst falsche Berichtserstattung, was ist die Intension?\n",
        "\n",
        "## Transformer Modelle\n",
        "Semantic - die Bedeutung von Wörtern im Kontext\n",
        "Transformer Modelle sind kleine Sprachmodelle, lernen Sprache im Kontext\n",
        "encoder - können Sprache verstehen aber nicht produzieren\n",
        "decoder - können Sprache verstehen und produzieren\n",
        "\n",
        "Vorhersehen welches Wort warscheinlich als nächstes kommt\n",
        "\n",
        "Hugging face ist ganz eng mit hugging face verbunden\n",
        "Modelle für Sentiment analyse via google auf hugging face suchen\n",
        "dann imprtieren, auf qualität des Modells achten"
      ],
      "metadata": {
        "id": "dwACYE_vURJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notizen Modul 5 - 5.12.25\n",
        "---\n",
        "### Wiederholung Sematic Search\n",
        "Algorythmus bestimmt ähnlichkeit von Wörtern darüber wie oft die Wörter im selben Kontext vorkommen - müssen nicht gemeinsam vorkommen, aber ähnlich genutzt werden, z.B. als synonym\n",
        "\n",
        "### Large Language Models\n",
        "100 000 mal so groß wie Semantic modules\n",
        "Woher kommen die Daten auf denen ChatGPT trainiert?\n",
        "- Wikipedia\n",
        "- Wikidata\n",
        "- GoogleBooks\n",
        "- jede halbwegs gute Quelle aus dem Internet (viel im Internet ist einfach scheiße)\n",
        "Verschiedene Trainingsschritte; Anfangstraining, Refinement, etc.\n",
        "Kontextlernen\n",
        "\n",
        "Huge Corpus --> Foundation Model --> Fine-Tuned LLM --> Aligned\n",
        "\n",
        "Foundation Model: kann nur vorhersagen  \n",
        "Fine-Tuned LLM: lernt zu sprechen\n",
        "\n",
        "#### LARGE Language Model Parameter\n",
        "Temperatur: Wie kreativ darf das Modell sein - bei 0 wählt das Modell immer die wahrscheinlichste Antwort, bei einer höheren Temperatur hat das modell mehr antwortmöglichkeiten\n",
        "   - die Wissenschaft nutzt of Temperaturen zwiischen 0 und 0.2 da die antworten so deterministischer und replikabler  \n",
        "Max-token: maximallänge der Antwort in Zeichen  \n",
        "top_k: maximale Anzahl der individuellen Tokens (Wörter,Zahlen,Buchstaben) die verwendet werden können  \n",
        "top_p: alternative zu top_k, z.B. nur die 20-30 häufigsten Tokens"
      ],
      "metadata": {
        "id": "vIC7wSEVz6kn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ideen für Term paper\n",
        "---\n",
        "* Frankfurter Zeitung\n",
        "* Wort basierte Analyse\n",
        "* Named Entity Recognition (M4) um Akteure zu ermitteln"
      ],
      "metadata": {
        "id": "d9dL7DxFO8Rn"
      }
    }
  ]
}